<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning">
  <meta name="keywords" content="mechanistic interpretability, in-context learning, large language models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>How do Large Language Models Learn In-Context? Query and Key
Matrices of In-Context Heads are Two Towers for Metric Learning</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://zepingyu0512.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://zepingyu0512.github.io/neuron-attribution.github.io/">
            Neuron-level knowledge attribution in LLM (EMNLP 2024 main)
          </a>
          <a class="navbar-item" href="https://zepingyu0512.github.io/arithmetic-mechanism.github.io/">
            Understanding arithmetic mechanism in LLM (EMNLP 2024 main)
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">How do Large Language Models Learn In-Context? 
            Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">EMNLP 2024 (main)</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://zepingyu0512.github.io/">Zeping Yu</a>,</span>
            <span class="author-block">
              <a href="https://research.manchester.ac.uk/en/persons/sophia.ananiadou">Sophia Ananiadou</a></span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">University of Manchester</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2402.02872"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2402.02872"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=9J3Z_7ilEec"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/zepingyu0512/in-context-mechanism"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1i91KB6IqNPQlCFyky_dI9C8Ui3qzJAer/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Slides</span>
                  </a>
              <span class="link-block">
                <a href="https://zhuanlan.zhihu.com/p/781421497"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Zhihu</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We investigate the mechanism of in-context
            learning (ICL) on sentence classification
            tasks with semantically-unrelated labels
            ("foo"/"bar"). We find intervening in only 1%
            heads (named "in-context heads") significantly
            affects ICL accuracy from 87.6% to 24.4%.
            To understand this phenomenon, we analyze
            the value-output vectors in these heads and
            discover that the vectors at each label position
            contain substantial information about the
            corresponding labels. Furthermore, we observe
            that the prediction shift from "foo" to "bar" is
            due to the respective reduction and increase in
            these heads’ attention scores at "foo" and "bar"
            positions. Therefore, we propose a hypothesis
            for ICL: in in-context heads, the value-output
            matrices extract label features, while the
            query-key matrices compute the similarity
            between the features at the last position and
            those at each label position. The query and
            key matrices can be considered as two towers
            that learn the similarity metric between the
            last position’s features and each demonstration
            at label positions. Using this hypothesis, we
            explain the majority label bias and recency
            bias in ICL and propose two methods to reduce
            these biases by 22% and 17%, respectively.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/9J3Z_7ilEec"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>

<section class="section">
  <div class="columns is-centered has-text-centered">
    <div class="container is-max-desktop"">
    <!-- Introduction. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
          In-context learning (ICL) is an emergent ability of large language models. By using some demonstration-label
          pairs as prompts, ICL performs well without updating parameters on many tasks. Because the mechanism of ICL 
          remains unclear, many studies focus on understanding how ICL works. Although previous studies are important 
          for understanding ICL, the exact mechanism of ICL remains a mystery for several reasons. 
          Firstly, the information flow is typically observed as an average across each head, but understanding ICL 
          requires exploring the precise importance of each head. Secondly, each head has a query matrix, key matrix, 
          value matrix, and output matrix; it is essential to study the role of each matrix in detail. Lastly, ICL is
          plagued by issues such as majority label bias and recency bias, and how to explain and mitigate these
          biases has not yet been thoroughly investigated.
          </p>
          <p>
          Contributions: In this paper, we <b>propose a hypothesis of in-context learning</b>, 
          and design experiments to <b>understand the roles of different modules (query, key, value, output matrices)</b>. 
          We understand and reduce the <b>majority label bias and recency bias</b> of in-context learning.
          </p>
        </div>
        <div class="column is-centered has-text-centered">
          <img src="./static/images/in-context.jpg"
               class="interpolation-image"
               alt="Interpolate start reference image."/>
          <p>Hypothesis of In-context Learning</p>
        </div>
        <br/>
      </div>
    </div>
    <!--/ Introduction. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Hypothesis of in-context learning -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Hypothesis of in-context learning</h2>
        <div class="content has-text-justified">
          <p>
          The proposed hypothesis is:
          In shallow layers, the label positions extract the demonstration features, and the last position extract 
          features in all positions (X% input text + Y% near demonstrations + Z% far demonstrations). 
          In deep layers' in-context heads, the value-output matrices extract the label features on the label 
          positions, learning foo->foo and bar->bar.
          The query and key matrices are two towers learning 
          the similarity metric between the last position and the demonstration features on the label positions. 
          When the similarity between the input text and the demonstration is large, the attention score on the 
          corresponding label position is large, thus the label information is transformed much into the last position
          , enhancing the probability of this label's token. Take <b>France : bar Cat : foo Dog : -> foo</b> as an example.
          <b>Sim(X% Dog + Y% Cat + Z% France, cat) > Sim(X% Dog + Y% Cat + Z% France, France)</b>, so foo is predicted.
          </p>
          <p>
          We list the experimental results supporting this hypothesis. Please find the details in the paper.
          </p>   
          <p>
          a) Previous studies find that label positions extract demonstrations' features.
          </p>   
          <p>
          b) We find a few <b>fooheads</b> important for predicting "foo", and a few <b>barheads</b> important for predicting "bar".
             When intervening the fooheads, the probability of "foo" decreases very much.  
             When intervening the barheads, the probability of "bar" decreases very much.  
             We name these heads <b>"in-context heads"</b>.
          </p>   
          <p>
          c) We design a logit minus score to evaluate the information storage in the weighted value-output vectors
             on each position. We find that the <b>"foo" positions in fooheads</b> store much information about "foo", and
             the <b>"bar" positions in barheads</b> store much information about "bar". So the mechanism of the in-context heads
             is a copying mechanism, similar to the induction head.
          </p>   
          <p>
          d) We compare the sentences <b>“S0 : bar S1 : bar S2 : foo S3 : foo S4 :” => foo</b> and 
             <b>“S0 : foo S1 : foo S2 : bar S3 : bar S4 :” => bar</b>. 
             The prediction changes from "foo" to "bar" when the labels are reversed.
             For each position in the in-context heads, 
             we compute the logit minus of the weighted value-output vectors, the attention scores, 
             and the logit minus of the value-output vectors. 
             We find that the <b>changing of attention scores is the root reason causing the probability change</b> from
             "foo" to "bar". In comparison, the value-output matrices learn foo->foo and bar->bar in both cases.
          </p>  
          <p>
          We also find mechanistic interpretability evidence of our hypothesis by projecting the vectors in unembedding
          space, shown below.
          </p>
        </div>
        <div class="column is-centered has-text-centered">
          <img src="./static/images/in-context-interpretability.jpg"
               class="interpolation-image"
               alt="Interpolate start reference image."/>
          <p>Mechanistic interpretability evidence in GPT2</p>
        </div>
      </div>
    </div>
    <!--/ Hypothesis of in-context learning -->
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{yu2024how,
  author    = {Yu, Zeping and Ananiadou, Sophia},
  title     = {How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning},
  journal   = {EMNLP},
  year      = {2024},
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/zepingyu0512" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-6">
        <div class="content">
          <p>
            This website is created using <a rel="license"
                                              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>
            under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
